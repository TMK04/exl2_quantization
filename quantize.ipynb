{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvdHZaR45hjm"
   },
   "source": [
    "# EXL2_Quantization\n",
    "\n",
    "Adapted from [Exllamav2 Quantization](https://colab.research.google.com/drive/1Cbb8nrwUxoxAbsIu1LLotsk2W52nj0Py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T05:28:31.766616Z",
     "iopub.status.busy": "2023-12-17T05:28:31.765805Z",
     "iopub.status.idle": "2023-12-17T05:28:31.771340Z",
     "shell.execute_reply": "2023-12-17T05:28:31.770236Z",
     "shell.execute_reply.started": "2023-12-17T05:28:31.766582Z"
    }
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "import re\n",
    "from safetensors.torch import load_file, save_file\n",
    "import shutil\n",
    "import subprocess\n",
    "import torch\n",
    "\n",
    "root_dir = Path(__file__).parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0dPo5ZV7KRq"
   },
   "source": [
    "1. Find an *unquantized* model repo in either `pytorch_model.bin` or `model.safetensors` format\n",
    "2. Replace the model author & name below\n",
    "    - e.g. `https://huggingface.co/{src_author}/{src_name}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T05:28:33.873865Z",
     "iopub.status.busy": "2023-12-17T05:28:33.873232Z",
     "iopub.status.idle": "2023-12-17T05:28:33.876924Z",
     "shell.execute_reply": "2023-12-17T05:28:33.876209Z",
     "shell.execute_reply.started": "2023-12-17T05:28:33.873843Z"
    }
   },
   "outputs": [],
   "source": [
    "src_author = \"\"\n",
    "src_name = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpw = 6. # Desired bits per weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-17T05:28:35.334081Z",
     "iopub.status.busy": "2023-12-17T05:28:35.333148Z",
     "iopub.status.idle": "2023-12-17T05:28:35.339947Z",
     "shell.execute_reply": "2023-12-17T05:28:35.338966Z",
     "shell.execute_reply.started": "2023-12-17T05:28:35.334057Z"
    }
   },
   "outputs": [],
   "source": [
    "models_dir = root_dir / \"models\"\n",
    "\n",
    "# Source model directory (`huggingface-cli download --local-dir` location)\n",
    "src_dir = os.path.join(models_dir, src_name)\n",
    "\n",
    "# Final output of quantized model\n",
    "dst_name = f\"{src_name}-{bpw:.1f}bpw-h6-exl2\"\n",
    "dst_dir = os.path.join(models_dir, dst_name)\n",
    "\n",
    "# Temporary directory for quantization\n",
    "quants_dir = root_dir / \"quants\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UW4c_4frcD72",
    "outputId": "7fc79c56-8eed-4197-fae4-b0b275411e3c"
   },
   "outputs": [],
   "source": [
    "exllama_dir = root_dir / \"exllamav2\"\n",
    "Path(src_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(quants_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(dst_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# # Download source model\n",
    "# subprocess.run([\"huggingface-cli\", \"download\", f\"{src_author}/{src_name}\", \"--local-dir\", src_dir, \"--local-dir-use-symlinks\", \"False\"])\n",
    "\n",
    "# # Download parquet\n",
    "# parquet_path = os.path.join(exllama_dir, \"0000.parquet\")\n",
    "# subprocess.run([\n",
    "#     \"wget\", \"-O\",\n",
    "#     parquet_path,\n",
    "#     r\"https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_70k/resolve/refs%2Fconvert%2Fparquet/default/train/0000.parquet\"\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVR88sxmkhqJ"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "**Convert to safetensors**\n",
    "\n",
    "Only run these next two cells\n",
    "if your model is in pytorch_model-00001-of-0000X.bin format\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-16T19:39:16.999601Z",
     "iopub.status.busy": "2023-12-16T19:39:16.998891Z",
     "iopub.status.idle": "2023-12-16T19:47:14.655106Z",
     "shell.execute_reply": "2023-12-16T19:47:14.654393Z",
     "shell.execute_reply.started": "2023-12-16T19:39:16.999601Z"
    },
    "id": "Uf4tbNC4qquL"
   },
   "outputs": [],
   "source": [
    "# Function to check file size\n",
    "def check_file_size(sf_filename: str, pt_filename: str):\n",
    "    sf_size = os.stat(sf_filename).st_size\n",
    "    pt_size = os.stat(pt_filename).st_size\n",
    "    if (sf_size - pt_size) / pt_size > 0.01:\n",
    "        raise RuntimeError(\n",
    "            f\"\"\"The file size difference is more than 1%:\n",
    "         - {sf_filename}: {sf_size}\n",
    "         - {pt_filename}: {pt_size}\n",
    "         \"\"\"\n",
    "        )\n",
    "\n",
    "# Function to convert individual file\n",
    "def convert_file(pt_filename: str, sf_filename: str):\n",
    "    loaded = torch.load(pt_filename, map_location=\"cpu\")\n",
    "    if \"state_dict\" in loaded:\n",
    "        loaded = loaded[\"state_dict\"]\n",
    "    loaded = {k: v.contiguous() for k, v in loaded.items()}\n",
    "    os.makedirs(os.path.dirname(sf_filename), exist_ok=True)\n",
    "    save_file(loaded, sf_filename, metadata={\"format\": \"pt\"})\n",
    "    check_file_size(sf_filename, pt_filename)\n",
    "    reloaded = load_file(sf_filename)\n",
    "    for k in loaded:\n",
    "        pt_tensor = loaded[k]\n",
    "        sf_tensor = reloaded[k]\n",
    "        if not torch.equal(pt_tensor, sf_tensor):\n",
    "            raise RuntimeError(f\"The output tensors do not match for key {k}\")\n",
    "\n",
    "def convert_all_files_in_directory(src_dir: str):\n",
    "    for filename in os.listdir(src_dir):\n",
    "        pt_filename = os.path.join(src_dir, filename)\n",
    "        sf_filename = None  # Initialize to None, will be set later if a match is found\n",
    "\n",
    "        # For files matching \"pytorch_model-(\\d+)-of-(\\d+).bin\"\n",
    "        match = re.match(r\"pytorch_model-(\\d+)-of-(\\d+).bin\", filename)\n",
    "        if match:\n",
    "            part_num, total_parts = match.groups()\n",
    "            sf_filename = os.path.join(src_dir, f\"model-{part_num.zfill(5)}-of-{total_parts.zfill(5)}.safetensors\")\n",
    "\n",
    "        # For files matching \"pytorch_model.bin\"\n",
    "        elif filename == \"pytorch_model.bin\":\n",
    "            sf_filename = os.path.join(src_dir, \"model.safetensors\")\n",
    "\n",
    "        # If a match was found, convert the file\n",
    "        if sf_filename:\n",
    "            convert_file(pt_filename, sf_filename)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    convert_all_files_in_directory(src_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oL2o6FA9tC4z"
   },
   "source": [
    "Deletes the pytorch_model.bin files to free up space after conversion\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-12-16T19:47:14.658135Z",
     "iopub.status.busy": "2023-12-16T19:47:14.657932Z",
     "iopub.status.idle": "2023-12-16T19:47:15.882183Z",
     "shell.execute_reply": "2023-12-16T19:47:15.881236Z",
     "shell.execute_reply.started": "2023-12-16T19:47:14.658121Z"
    },
    "id": "4Hq3L2_4sIfV",
    "outputId": "09a777bf-e2c9-401c-be37-f6a8a9d4c356"
   },
   "outputs": [],
   "source": [
    "# Function to delete all .bin files in a src_dir\n",
    "def delete_all_bin_files_in_directory(src_dir: str):\n",
    "    for filename in os.listdir(src_dir):\n",
    "        match = re.match(r\"pytorch_model-(\\d+)-of-(\\d+).bin\", filename)\n",
    "        if match:\n",
    "            file_path = os.path.join(src_dir, filename)\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted {file_path}\")\n",
    "\n",
    "# Run the deletion\n",
    "delete_all_bin_files_in_directory(src_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjQ1NZCxqq8X"
   },
   "source": [
    "## Quantize\n",
    "\n",
    "[Documentation here](https://github.com/turboderp/exllamav2/blob/master/doc/convert.md)\n",
    "\n",
    "Choose your BPW above @ [Configuration](#configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-12-17T05:28:42.839390Z",
     "iopub.status.busy": "2023-12-17T05:28:42.838488Z",
     "iopub.status.idle": "2023-12-17T10:23:15.304157Z",
     "shell.execute_reply": "2023-12-17T10:23:15.303270Z",
     "shell.execute_reply.started": "2023-12-17T05:28:42.839359Z"
    },
    "id": "bbpGNtxrehTZ",
    "outputId": "072c9606-7aeb-413b-d56c-68c4448bbc3e"
   },
   "outputs": [],
   "source": [
    "def run_command_and_stream_output(command, cwd):\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, cwd=cwd)\n",
    "    while True:\n",
    "        output = process.stdout.readline()\n",
    "        if output == '' and process.poll() is not None:\n",
    "            break\n",
    "        if output:\n",
    "            print(output.strip())\n",
    "\n",
    "command = [\n",
    "    \"python\",\n",
    "    \"convert.py\",\n",
    "    \"-i\", src_dir,\n",
    "    \"-o\", quants_dir,\n",
    "    \"-c\", parquet_path,\n",
    "    \"-cf\", dst_dir,\n",
    "    \"-b\", bpw\n",
    "]\n",
    "\n",
    "run_command_and_stream_output(command, exllama_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GZBvGVa5DtP"
   },
   "source": [
    "Renames all output(s).safetensors to model(s).safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2023-12-17T10:40:23.800793Z",
     "iopub.status.busy": "2023-12-17T10:40:23.800193Z",
     "iopub.status.idle": "2023-12-17T10:40:23.811254Z",
     "shell.execute_reply": "2023-12-17T10:40:23.810702Z",
     "shell.execute_reply.started": "2023-12-17T10:40:23.800772Z"
    },
    "id": "JDqWAN9z4IWI",
    "outputId": "a8e56048-d3c7-447f-aa9e-e917a35d7c8b"
   },
   "outputs": [],
   "source": [
    "for f in glob(os.path.join(dst_dir, \"output*.safetensors\")):\n",
    "  new_name = f.replace(\"output\", \"model\")\n",
    "  os.rename(f, new_name)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
